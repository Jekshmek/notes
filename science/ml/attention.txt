2019
https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html
https://habr.com/ru/post/436878/
2018
Attention Is All You Need
    https://www.youtube.com/watch?v=iDulhoQ2pro
    https://arxiv.org/abs/1706.03762

http://nlp.seas.harvard.edu/2018/04/03/attention.html
https://mchromiak.github.io/articles/2017/Sep/12/Transformer-Attention-is-all-you-need/#.W-2AUeK-mUk
https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html

BERT
https://towardsdatascience.com/deconstructing-bert-distilling-6-patterns-from-100-million-parameters-b49113672f77
https://jalammar.github.io/illustrated-bert/
https://medium.com/dissecting-bert/dissecting-bert-appendix-the-decoder-3b86f66b0e5f
https://medium.com/dissecting-bert/dissecting-bert-part2-335ff2ed9c73
https://medium.com/dissecting-bert/dissecting-bert-part-1-d3c3d495cdb3
https://www.infoq.com/news/2018/11/google-bert-nlp
https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html
https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html
https://github.com/google-research/bert/
BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding
    https://www.youtube.com/watch?v=-9evrZnBorM
    https://arxiv.org/abs/1810.04805

https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270

https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/

seq2seq
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

LSTMs
http://colah.github.io/posts/2015-08-Understanding-LSTMs/
